{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template 提示词模板"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt template  字符模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你是一个AI模型专家'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 字符模板\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"你是一个{area}专家\")\n",
    "prompt.format(area=\"AI模型\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对话模板 ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='I am an AI model.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello my AI!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ai_response', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "   (\"system\", \"You are a helpful assistant.\"),\n",
    "   (\"ai\", \"I am an AI model.\"),\n",
    "   (\"human\", \"{user_input}\"),\n",
    "   (\"assistant\", \"ai_response\"),\n",
    "  #  (\"xxxx\", \"JJJJJJ\")\n",
    "  # Unexpected message type: xxxx.\n",
    "  # Use one of 'human', 'user', 'ai', 'assistant', or 'system'.\n",
    "])\n",
    "chat_template.format_messages(user_input=\"Hello my AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={'name': 'system'}, response_metadata={}),\n",
       " HumanMessage(content='You are a helpful assistant.', additional_kwargs={'name': 'human'}, response_metadata={}),\n",
       " AIMessage(content='I am an AI model.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "sm = SystemMessage(content='You are a helpful assistant.', additional_kwargs={'name': 'system'})\n",
    "h_m = HumanMessage(content='You are a helpful assistant.', additional_kwargs={'name': 'human'})\n",
    "ai_m = AIMessage(content='I am an AI model.')\n",
    "[sm, h_m, ai_m]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatMassagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(content='愿DDDD荣光永存！', additional_kwargs={}, response_metadata={}, role='father')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import AIMessagePromptTemplate\n",
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "pt = \"愿{obj}荣光永存！\"\n",
    "\n",
    "sys_template = SystemMessagePromptTemplate.from_template(template=pt)\n",
    "sys_template.format(obj=\"AAA\")\n",
    "\n",
    "hum_template = HumanMessagePromptTemplate.from_template(template=pt)\n",
    "hum_template.format(obj=\"BBB\")\n",
    "\n",
    "ai_template = AIMessagePromptTemplate.from_template(template=pt)\n",
    "ai_template.format(obj=\"CCCC\")\n",
    "\n",
    "fat_template = ChatMessagePromptTemplate.from_template(role='father', template=pt)\n",
    "fat_template.format(obj=\"DDDD\")\n",
    "\n",
    "# [sys_template, hum_template, ai_template, fat_template]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义模板 StringPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "嗯，我现在要解析一个函数。这个函数的定义是def add(x: int, y: int) -> int: return x + y。首先，我需要确定每个部分的具体信息。\n",
      "\n",
      "第一步是函数名，很明显是“add”，所以参数列表的第一点应该是“函数名：add”。\n",
      "\n",
      "接下来是参数列表，这里有两个参数x和y，都取int类型，并且用typed的 syntax标记，也就是x: int 和 y: int。所以我应该把它们放在参数列表中，用逗号分隔。\n",
      "\n",
      "然后是返回值类型，函数返回的是int，所以第三点应该是“返回值类型：int”。\n",
      "\n",
      "接下来是函数体，这部分很重要。我需要写出完整的def语句，包括参数的定义、括号和返回部分。所以第二点是：\n",
      "def add(x: int, y: int) -> int:\n",
      "然后是return x + y。\n",
      "\n",
      "现在要解释这个函数的功能，也就是它能做什么。这里函数add接受两个整数x和y，并将它们相加，输出结果。比如，当x是3，y是5时，返回8。\n",
      "\n",
      "最后是示例代码，用来验证函数是否正确。比如：\n",
      "print(add(3, 5)) 应该会输出8。\n",
      "\n",
      "总结一下，我需要按照这些步骤来组织和解释每个部分的信息。\n",
      "</think>\n",
      "\n",
      "函数解析如下：\n",
      "\n",
      "- 1. 函数名：add\n",
      "- 2. 参数列表：\n",
      "  - x: int\n",
      "  - y: int\n",
      "- 3. 返回值类型：int\n",
      "- 4. 函数体：\n",
      "  def add(x: int, y: int) -> int:\n",
      "    return x + y\n",
      "\n",
      "- 5. 功能解释：\n",
      "  这个函数定义了一个名为add的函数，该函数接受两个参数x和y，这两个参数都是整数类型，并且返回它们的和。它是一个简单的加法函数。\n",
      "\n",
      "- 6. 示例代码：\n",
      "  print(add(3, 5))  \n",
      "  # 输出：8\n"
     ]
    }
   ],
   "source": [
    "# AI函数解析器\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "\n",
    "# 定义一个简单的函数\n",
    "def add(x: int, y: int) -> int:\n",
    "    return x + y\n",
    "\n",
    "# 创建一个自定义提示词模板\n",
    "PROMPT=\"\"\"\\\n",
    "你是一个函数解析器, 可以解析函数的参数和返回值。\n",
    "根据给定的函数内容:{function_body},解析出一下内容：\n",
    "参数列表, 返回值类型, 函数功能解释, 示例代码。\n",
    "按照以下格式输出:\n",
    "\n",
    "函数解析如下：\n",
    "- 1. 函数名：{function_name}\n",
    "- 2. 参数列表：\n",
    "- 3. 返回值类型：\n",
    "- 4. 函数体：\n",
    "     {function_body}\n",
    "- 5. 功能解释：\n",
    "- 6. 示例代码：\n",
    "\"\"\"\n",
    "\n",
    "import inspect\n",
    "\n",
    "def get_function_source(func):\n",
    "    # 获取函数信息\n",
    "    return inspect.getsource(func)\n",
    "\n",
    "\n",
    "# 创建一个自定义的提示词模板类\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    def format(self, **kwargs):\n",
    "        # 获得源代码信息\n",
    "        func = kwargs.pop(\"function\")\n",
    "        func_info = get_function_source(func)\n",
    "        return PROMPT.format(\n",
    "            function_name=func.__name__,\n",
    "            function_body=func_info,\n",
    "        )\n",
    "\n",
    "\n",
    "# 创建一个StringPromptTemplate实例\n",
    "custom_pt = CustomPromptTemplate(input_variables=[\"function\"])\n",
    "cpt_str = custom_pt.format(function=add)\n",
    "\n",
    "# 将生成的模板输入给AI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 指定本地模型名称或路径\n",
    "ollama_llm = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\",  # 例如DeepSeek-R1 1.5B模型\n",
    "    base_url=\"http://localhost:11434\",  # Ollama默认地址\n",
    "    temperature=0,\n",
    ")\n",
    "print(ollama_llm.predict(cpt_str))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
