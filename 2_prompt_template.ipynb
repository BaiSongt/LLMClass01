{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template 提示词模板"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt template  字符模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你是一个AI模型专家'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 字符模板\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"你是一个{area}专家\")\n",
    "prompt.format(area=\"AI模型\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对话模板 ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='I am an AI model.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello my AI!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ai_response', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "   (\"system\", \"You are a helpful assistant.\"),\n",
    "   (\"ai\", \"I am an AI model.\"),\n",
    "   (\"human\", \"{user_input}\"),\n",
    "   (\"assistant\", \"ai_response\"),\n",
    "  #  (\"xxxx\", \"JJJJJJ\")\n",
    "  # Unexpected message type: xxxx.\n",
    "  # Use one of 'human', 'user', 'ai', 'assistant', or 'system'.\n",
    "])\n",
    "chat_template.format_messages(user_input=\"Hello my AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={'name': 'system'}, response_metadata={}),\n",
       " HumanMessage(content='You are a helpful assistant.', additional_kwargs={'name': 'human'}, response_metadata={}),\n",
       " AIMessage(content='I am an AI model.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "sm = SystemMessage(content='You are a helpful assistant.', additional_kwargs={'name': 'system'})\n",
    "h_m = HumanMessage(content='You are a helpful assistant.', additional_kwargs={'name': 'human'})\n",
    "ai_m = AIMessage(content='I am an AI model.')\n",
    "[sm, h_m, ai_m]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatMassagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(content='愿DDDD荣光永存！', additional_kwargs={}, response_metadata={}, role='father')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.prompts import AIMessagePromptTemplate\n",
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "pt = \"愿{obj}荣光永存！\"\n",
    "\n",
    "sys_template = SystemMessagePromptTemplate.from_template(template=pt)\n",
    "sys_template.format(obj=\"AAA\")\n",
    "\n",
    "hum_template = HumanMessagePromptTemplate.from_template(template=pt)\n",
    "hum_template.format(obj=\"BBB\")\n",
    "\n",
    "ai_template = AIMessagePromptTemplate.from_template(template=pt)\n",
    "ai_template.format(obj=\"CCCC\")\n",
    "\n",
    "fat_template = ChatMessagePromptTemplate.from_template(role='father', template=pt)\n",
    "fat_template.format(obj=\"DDDD\")\n",
    "\n",
    "# [sys_template, hum_template, ai_template, fat_template]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义模板 StringPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "嗯，我现在要解析一个Python函数。这个函数的定义是这样的：def add(x: int, y: int) -> int: return x + y。首先，我需要理解每个部分的意思。\n",
      "\n",
      "函数名是add，这很简单，直接写上“add”。接下来是参数列表，里面有x和y两个参数，都是int类型。所以参数列表应该包括这两个变量，并且说明它们的类型。\n",
      "\n",
      "然后是返回值类型，函数体最后面有个-> int，表示这个函数返回的是一个整数。这部分需要明确，不能出错。\n",
      "\n",
      "接下来是函数的定义部分，也就是def add(x: int, y: int) -> int:. 这里x和y都是int类型的参数，并且整个函数返回的是int类型的结果。然后是函数体，直接写return x + y，这是简单的加法运算。\n",
      "\n",
      "现在，我需要把这些信息按照用户的要求整理出来，按照指定的格式输出。首先列出每个部分的内容，确保不遗漏任何一个细节。\n",
      "\n",
      "最后，检查一下是否所有部分都覆盖到了，有没有可能的地方被漏掉或者描述不清楚的地方。比如参数类型、返回值类型、函数名这些都很明确，没有问题。\n",
      "\n",
      "总结一下，我需要按照用户给定的示例格式来填写，确保每个部分的信息准确无误地呈现出来。\n",
      "</think>\n",
      "\n",
      "函数解析如下：\n",
      "\n",
      "- 1. 函数名：add\n",
      "- 2. 参数列表：\n",
      "- 3. 返回值类型：int\n",
      "- 4. 函数体：\n",
      "    def add(x: int, y: int) -> int:\n",
      "        return x + y\n",
      "- 5. 功能解释：这个函数接受两个整数参数x和y，并返回它们的和。\n",
      "- 6. 示例代码：\n",
      "    a = 3\n",
      "    b = 4\n",
      "    result = add(a, b)\n",
      "    print(result)  # 输出是7\n"
     ]
    }
   ],
   "source": [
    "# AI函数解析器\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "\n",
    "# 定义一个简单的函数\n",
    "def add(x: int, y: int) -> int:\n",
    "    return x + y\n",
    "\n",
    "# 创建一个自定义提示词模板\n",
    "PROMPT=\"\"\"\\\n",
    "你是一个函数解析器, 可以解析AI函数的参数和返回值。\n",
    "根据给定的函数内容:{function_body},解析出一下内容：\n",
    "参数列表, 返回值类型, 函数功能解释, 示例代码。\n",
    "按照以下格式输出:\n",
    "\n",
    "函数解析如下：\n",
    "- 1. 函数名：{function_name}\n",
    "- 2. 参数列表：\n",
    "- 3. 返回值类型：\n",
    "- 4. 函数体：\n",
    "     {function_body}\n",
    "- 5. 功能解释：\n",
    "- 6. 示例代码：\n",
    "\"\"\"\n",
    "\n",
    "import inspect\n",
    "\n",
    "def get_function_source(func):\n",
    "    # 获取函数信息\n",
    "    return inspect.getsource(func)\n",
    "\n",
    "\n",
    "# 创建一个自定义的提示词模板类\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    def format(self, **kwargs):\n",
    "        # 获得源代码信息\n",
    "        func = kwargs.pop(\"function\")\n",
    "        func_info = get_function_source(func)\n",
    "        return PROMPT.format(\n",
    "            function_name=func.__name__,\n",
    "            function_body=func_info,\n",
    "        )\n",
    "\n",
    "\n",
    "# 创建一个StringPromptTemplate实例\n",
    "custom_pt = CustomPromptTemplate(input_variables=[\"function\"])\n",
    "cpt_str = custom_pt.format(function=add)\n",
    "\n",
    "# 将生成的模板输入给AI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 指定本地模型名称或路径\n",
    "ollama_llm = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\",  # 例如DeepSeek-R1 1.5B模型\n",
    "    base_url=\"http://localhost:11434\",  # Ollama默认地址\n",
    "    temperature=0.5,\n",
    ")\n",
    "print(ollama_llm.predict(cpt_str))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
