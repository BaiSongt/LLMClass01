{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain 中的 **MapReduceChain** 是一种用于处理长文本或大量网页的摘要生成方法，其核心原理是通过分块处理和多级摘要来突破大模型上下文长度的限制。以下是其原理、总结及使用思路的详细说明：\n",
    "\n",
    "---\n",
    "\n",
    "### **原理**\n",
    "1. **分块处理（Map 阶段）**  \n",
    "   将输入的网页内容分割成多个小块（chunk），每个小块独立提交给大模型生成初步摘要。这一过程通过 `Map` 操作实现，每个分块的处理结果作为中间摘要。\n",
    "\n",
    "2. **合并摘要（Reduce 阶段）**  \n",
    "   将所有中间摘要组合成一个新的提示词（prompt），再次提交给大模型进行最终摘要生成。这一过程通过 `Reduce` 操作实现，目的是整合各分块的关键信息，形成连贯的总结。\n",
    "\n",
    "3. **Token 优化**  \n",
    "   相比直接将全文输入模型（Stuff 方法），MapReduceChain 通过两次摘要减少了单次请求的 Token 开销，但需注意多级处理可能增加整体计算成本。\n",
    "\n",
    "---\n",
    "\n",
    "### **总结**\n",
    "- **优势**：  \n",
    "  - 适用于长网页或大量网页的摘要，避免超出模型上下文限制。  \n",
    "  - 通过分块和合并机制，平衡了处理效率与摘要质量。  \n",
    "\n",
    "- **局限性**：  \n",
    "  - 多级摘要可能导致信息冗余或关键细节丢失。  \n",
    "  - Token 开销随分块数量增加而上升，需权衡分块大小。  \n",
    "\n",
    "---\n",
    "\n",
    "### **使用思路**\n",
    "1. **初始化组件**  \n",
    "   - 加载大模型（如 `ChatOpenAI`）。  \n",
    "   - 加载网页内容（如使用 `UnstructuredFileLoader` 或 `WebBaseLoader`）。  \n",
    "\n",
    "2. **选择链类型**  \n",
    "   使用 `load_summarize_chain` 函数并指定 `chain_type=\"map_reduce\"`，LangChain 会自动构建 MapReduce 链。  \n",
    "\n",
    "3. **运行链**  \n",
    "   调用 `chain.run(docs)` 执行摘要生成，输出结果为最终合并的摘要文本。  \n",
    "\n",
    "4. **代码示例**  \n",
    "   ```python\n",
    "   from langchain.chat_models import ChatOpenAI\n",
    "   from langchain.document_loaders import UnstructuredFileLoader\n",
    "   from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "   # 初始化模型和网页\n",
    "   loader = UnstructuredFileLoader(\"./large_document.pdf\")\n",
    "   docs = loader.load()\n",
    "   llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "   # 创建 MapReduce 链\n",
    "   chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "   # 生成摘要\n",
    "   summary = chain.run(docs)\n",
    "   print(summary[\"output_text\"])\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **与其他链类型的对比**\n",
    "- **Stuff**：直接将全文输入模型，简单但易超限，适合小网页。  \n",
    "- **Refine**：逐块生成摘要并迭代合并，Token 开销更低，但可能丢失早期信息。  \n",
    "\n",
    "根据场景需求（如网页长度、Token 限制、摘要质量），选择合适的链类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages (4.50.3)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = OllamaLLM(model=\"deepseek-r1:1.5b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 211, which is longer than the specified 200\n",
      "Created a chunk of size 218, which is longer than the specified 200\n",
      "Created a chunk of size 270, which is longer than the specified 200\n",
      "Created a chunk of size 254, which is longer than the specified 200\n",
      "Created a chunk of size 242, which is longer than the specified 200\n",
      "Created a chunk of size 381, which is longer than the specified 200\n"
     ]
    }
   ],
   "source": [
    "# Load txt\n",
    "loader = TextLoader(\"long_text.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# split text\n",
    "# Initialize the text splitter with appropriate chunk size and overlap\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=10,\n",
    "    separator=\"\\n\",  # Optional: Specify a separator for better splitting\n",
    "    add_start_index=True  # Optional: Include start indices for tracking\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "# print(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map chain\n",
    "map_template = \"\"\"对以下内容做简要总结:\n",
    "\"{content}\"\n",
    "总结内容:\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    template=map_template,\n",
    ")\n",
    "\n",
    "map_chain = LLMChain(\n",
    "  llm = llm,\n",
    "  prompt = map_prompt,\n",
    "  # verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce chain\n",
    "reduce_template = \"\"\"以下是一个摘要集合:\n",
    "\"{summary_content}\"\n",
    "将上述摘要与所有关键细节进行总结。\n",
    "总结:\"\"\"\n",
    "reduce_prompt = PromptTemplate(\n",
    "    input_variables=[\"summary_content\"],\n",
    "    template=reduce_template,\n",
    ")\n",
    "reduce_chain = LLMChain(\n",
    "  llm = llm,\n",
    "  prompt = reduce_prompt,\n",
    "  # verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stuff chain\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain,\n",
    "    document_variable_name=\"summary_content\",\n",
    "    # verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce final chain\n",
    "reduce_final_chain = ReduceDocumentsChain(\n",
    "    combine_documents_chain=stuff_chain,\n",
    "    collapse_documents_chain=stuff_chain,\n",
    "    token_max=200,\n",
    "    # verbose=True\n",
    ")\n",
    "\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "  llm_chain=map_chain,\n",
    "  document_variable_name=\"content\",\n",
    "  reduce_documents_chain=reduce_final_chain,\n",
    "  verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m summary = \u001b[43mmap_reduce_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain_core/_api/deprecation.py:181\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    180\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py:606\u001b[39m, in \u001b[36mChain.run\u001b[39m\u001b[34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[39m\n\u001b[32m    604\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) != \u001b[32m1\u001b[39m:\n\u001b[32m    605\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`run` supports only one positional argument.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[32m    607\u001b[39m         _output_key\n\u001b[32m    608\u001b[39m     ]\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    611\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[32m    612\u001b[39m         _output_key\n\u001b[32m    613\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain_core/_api/deprecation.py:181\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    180\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py:389\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    358\u001b[39m \n\u001b[32m    359\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    380\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    381\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    382\u001b[39m config = {\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    384\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    385\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    386\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    387\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py:170\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    169\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    171\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py:160\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    159\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    163\u001b[39m     )\n\u001b[32m    165\u001b[39m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    166\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    167\u001b[39m     )\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain/chains/combine_documents/base.py:138\u001b[39m, in \u001b[36mBaseCombineDocumentsChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[32m    137\u001b[39m other_keys = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items() \u001b[38;5;28;01mif\u001b[39;00m k != \u001b[38;5;28mself\u001b[39m.input_key}\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m output, extra_return_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_run_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mother_keys\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m extra_return_dict[\u001b[38;5;28mself\u001b[39m.output_key] = output\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain/chains/combine_documents/map_reduce.py:240\u001b[39m, in \u001b[36mMapReduceDocumentsChain.combine_docs\u001b[39m\u001b[34m(self, docs, token_max, callbacks, **kwargs)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcombine_docs\u001b[39m(\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    230\u001b[39m     docs: List[Document],\n\u001b[32m   (...)\u001b[39m\u001b[32m    233\u001b[39m     **kwargs: Any,\n\u001b[32m    234\u001b[39m ) -> Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m    235\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[32m    236\u001b[39m \n\u001b[32m    237\u001b[39m \u001b[33;03m    Combine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[33;03m    This reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     map_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# FYI - this is parallelized and so it is fast.\u001b[39;49;00m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     question_result_key = \u001b[38;5;28mself\u001b[39m.llm_chain.output_key\n\u001b[32m    246\u001b[39m     result_docs = [\n\u001b[32m    247\u001b[39m         Document(page_content=r[question_result_key], metadata=docs[i].metadata)\n\u001b[32m    248\u001b[39m         \u001b[38;5;66;03m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[32m    249\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(map_results)\n\u001b[32m    250\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain/chains/llm.py:251\u001b[39m, in \u001b[36mLLMChain.apply\u001b[39m\u001b[34m(self, input_list, callbacks)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    250\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    252\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.create_outputs(response)\n\u001b[32m    253\u001b[39m run_manager.on_chain_end({\u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: outputs})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain/chains/llm.py:248\u001b[39m, in \u001b[36mLLMChain.apply\u001b[39m\u001b[34m(self, input_list, callbacks)\u001b[39m\n\u001b[32m    242\u001b[39m run_manager = callback_manager.on_chain_start(\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    244\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33minput_list\u001b[39m\u001b[33m\"\u001b[39m: input_list},\n\u001b[32m    245\u001b[39m     name=\u001b[38;5;28mself\u001b[39m.get_name(),\n\u001b[32m    246\u001b[39m )\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    250\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain/chains/llm.py:138\u001b[39m, in \u001b[36mLLMChain.generate\u001b[39m\u001b[34m(self, input_list, run_manager)\u001b[39m\n\u001b[32m    136\u001b[39m callbacks = run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm, BaseLanguageModel):\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    145\u001b[39m     results = \u001b[38;5;28mself\u001b[39m.llm.bind(stop=stop, **\u001b[38;5;28mself\u001b[39m.llm_kwargs).batch(\n\u001b[32m    146\u001b[39m         cast(List, prompts), {\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks}\n\u001b[32m    147\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain_core/language_models/llms.py:763\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    756\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    757\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[32m   (...)\u001b[39m\u001b[32m    760\u001b[39m     **kwargs: Any,\n\u001b[32m    761\u001b[39m ) -> LLMResult:\n\u001b[32m    762\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain_core/language_models/llms.py:966\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    952\u001b[39m     run_managers = [\n\u001b[32m    953\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    954\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    964\u001b[39m         )\n\u001b[32m    965\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m    970\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain_core/language_models/llms.py:787\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    777\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    778\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    779\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    783\u001b[39m     **kwargs: Any,\n\u001b[32m    784\u001b[39m ) -> LLMResult:\n\u001b[32m    785\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    786\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    791\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    795\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    796\u001b[39m         )\n\u001b[32m    797\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    798\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain_ollama/llms.py:288\u001b[39m, in \u001b[36mOllamaLLM._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m generations = []\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m     generations.append([final_chunk])\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations=generations)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain_ollama/llms.py:256\u001b[39m, in \u001b[36mOllamaLLM._stream_with_aggregation\u001b[39m\u001b[34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_stream_with_aggregation\u001b[39m(\n\u001b[32m    248\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    249\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    253\u001b[39m     **kwargs: Any,\n\u001b[32m    254\u001b[39m ) -> GenerationChunk:\n\u001b[32m    255\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_generate_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mGenerationChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m                \u001b[49m\u001b[43mgeneration_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    262\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain_ollama/llms.py:211\u001b[39m, in \u001b[36mOllamaLLM._create_generate_stream\u001b[39m\u001b[34m(self, prompt, stop, **kwargs)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_generate_stream\u001b[39m(\n\u001b[32m    206\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    207\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    208\u001b[39m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    209\u001b[39m     **kwargs: Any,\n\u001b[32m    210\u001b[39m ) -> Iterator[Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.generate(\n\u001b[32m    212\u001b[39m         **\u001b[38;5;28mself\u001b[39m._generate_params(prompt, stop=stop, **kwargs)\n\u001b[32m    213\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/ollama/_client.py:170\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    167\u001b[39m   e.response.read()\n\u001b[32m    168\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m  \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43merror\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/httpx/_models.py:929\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    927\u001b[39m decoder = LineDecoder()\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/httpx/_models.py:916\u001b[39m, in \u001b[36mResponse.iter_text\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    914\u001b[39m chunker = TextChunker(chunk_size=chunk_size)\n\u001b[32m    915\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/httpx/_models.py:897\u001b[39m, in \u001b[36mResponse.iter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    895\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/httpx/_models.py:951\u001b[39m, in \u001b[36mResponse.iter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    948\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m951\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_num_bytes_downloaded\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/httpx/_client.py:153\u001b[39m, in \u001b[36mBoundSyncStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/httpx/_transports/default.py:127\u001b[39m, in \u001b[36mResponseStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpcore_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/httpcore/_sync/http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/httpcore/_sync/http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/httpcore/_sync/http11.py:203\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    200\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "summary = map_reduce_chain.run(split_docs)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重排序（Reranking）技术\n",
    "在 RAG（检索增强生成）系统中，MapReduceChain 可与 ​重排序（Reranking）​ 技术结合，进一步提升摘要质量：\n",
    "\n",
    "### ***​检索阶段***\n",
    "使用向量数据库（如 FAISS）检索相关网页块，生成候选摘要列表。\n",
    "### ***​重排序阶段***\n",
    "通过重排序模型（如 BCEmbedding 或 LLM）对候选摘要进行二次排序，筛选出语义最相关的结果。\n",
    "### ***​生成阶段***\n",
    "将优化后的摘要输入 MapReduceChain，生成最终答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 381, which is longer than the specified 200\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "llm = ChatOllama(\n",
    "  model=\"deepseek-r1:1.5b\",\n",
    "  temperature=0\n",
    ")\n",
    "\n",
    "# load text\n",
    "loader = TextLoader(\"long_text.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# split document\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=10,\n",
    "    separator=\"\\n\",  # Optional: Specify a separator for better splitting\n",
    "    add_start_index=True  # Optional: Include start indices for tracking\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load_qa_with_sources_chain\n",
    "**chain_type=\"map_rerank\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = load_qa_with_sources_chain(\n",
    "  llm = llm,\n",
    "  chain_type=\"map_rerank\",\n",
    "  verbose=True,\n",
    "  metadata_keys = [\"source\"],\n",
    "  return_intermediate_steps = True\n",
    ")\n",
    "\n",
    "# print(qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "{context}\n",
      "---------\n",
      "Question: {question}\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "  \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nIn addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\\n\\nQuestion: [question here]\\nHelpful Answer: [answer here]\\nScore: [score between 0 and 100]\\n\\nHow to determine the score:\\n- Higher is a better answer\\n- Better responds fully to the asked question, with sufficient level of detail\\n- If you do not know the answer based on the context, that should be a score of 0\\n- Don't be overconfident!\\n\\nExample #1\\n\\nContext:\\n---------\\nApples are red\\n---------\\nQuestion: what color are apples?\\nHelpful Answer: red\\nScore: 100\\n\\nExample #2\\n\\nContext:\\n---------\\nit was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\\n---------\\nQuestion: what type was the car?\\nHelpful Answer: a sports car or an suv\\nScore: 60\\n\\nExample #3\\n\\nContext:\\n---------\\nPears are either red or orange\\n---------\\nQuestion: what color are apples?\\nHelpful Answer: This document does not answer the question\\nScore: 0\\n\\nBegin!\\n\\nContext:\\n---------\\n{context}\\n---------\\nQuestion: {question}\\nHelpful Answer:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain/chains/llm.py:369: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "作为全球最大的中文搜索引擎公司，百度一直致力于让网民更平等的获取信息，找到所求。百度是用户获取信息的最主要入口，随着移动互联网的发展，百度网页搜索完成了由PC向移动的转型，由连接人与信息扩展到连接人与服务，用户可以在PC、Pad、手机上访问百度主页，通过文字、语音、图像多种交互方式瞬间找到所需要的信息和服务。\n",
      "百度（百度App）：7亿用户首选的搜索和资讯客户端\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "百度App是一款有7亿用户在使用的手机“搜索+资讯”客户端，结合了搜索功能和智能信息推荐，依托百度网页、百度图片、百度新闻、百度知道、百度百科、百度地图、百度音乐、百度视频等专业垂直频道“有事搜一搜，没事看一看”，为用户提供更多丰富和实用的功能与服务。\n",
      "百度地图：人工智能时代新地图\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "百度地图（Baidu Maps）自2005年上线以来，秉持“科技让出行更简单”的品牌使命，以\"科技\"为手段不断探索创新，已经发展成为国内领先的互联网地图服务商。2024年4月，百度地图发布全新手车一体V20版本。包含四大核心能力：手车一体、真车道级导航、地图智能体、手车互联。其中，百度地图车道级导航已实现全国覆盖。 [164]\n",
      "百度，连接人与服务\n",
      "百度糯米：省钱更省心！\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "百度糯米汇集美食、电影、酒店、休闲娱乐、旅游、到家服务等众多生活服务的相关产品，并先后接入百度外卖、去哪儿网资源，一站式解决吃喝玩乐相关的所有问题，逐渐完善了百度糯米O2O的生态布局。\n",
      "百度，每个人的舞台\n",
      "百度贴吧：上贴吧，找组织\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "百度贴吧，全球最大的中文社区。贴吧是一种基于关键词的主题交流社区，它与搜索紧密结合，准确把握用户需求，搭建别具特色的“兴趣主题“互动平台。贴吧目录涵盖社会、地区、生活、教育、娱乐明星、游戏、体育、企业等方方面面，目前是全球最大的中文交流平台。\n",
      "百度百科：全球最大的中文百科全书\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "百度百科是一个内容开放、自由的网络百科全书平台， 旨在创造一个涵盖各领域知识的中文信息收集平台。百度百科强调用户的参与和奉献精神，充分调动互联网用户的力量，汇聚上亿用户的头脑智慧，积极进行交流和分享。\n",
      "百度知道：总有一个人知道你问题的答案\n",
      "百度知道，是百度旗下的互动式知识问答分享平台，也是全球最大的中文问答平台。广大网友根据实际需求在百度知道上进行提问，便立即获得数亿网友的在线解答。\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "百度文库：让每个人平等的提升自我\n",
      "百度文库是百度发布的供网友在线分享文档的知识平台，是最大的互联网学习开放平台。百度文库用户可以在此平台上，上传， 在线阅读与下载文档。\n",
      "百度健康：你身边的健康管家\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "百度健康是百度移动生态体系中最重要的垂类之一，是百度自身孵化和打造的一站式健康管理平台，联合顶级医疗资源，构建健康知识服务、在线医疗咨询服务、健康商城服务、慢病管理服务及互联网医院服务五大体系，让用户便捷地获取可靠的健康知识和优质的健康服务。截至2021年8月，百度健康已收录权威科普内容5亿条，吸引超过30万专业医生入驻，每日提供在线医疗咨询服务超过200万次。\n",
      "好看视频：分享美好，看见世界\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "好看视频平台拥有独家短视频内容源，分类覆盖搞笑、音乐、影视、娱乐、游戏、生活、小品、军事、汽车、新闻等全方位优质视频内容，是一个专业短视频聚合平台。数十万视频创作者通过好看视频给7亿百度生态用户提供全方位的视频内容，每天的观看次数高达数十亿次。\n",
      "百度，互联网生活助手\n",
      "小度：无处不在的人工智能个人助手\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "百度是中国对话式人工智能的开创者，从2015年百度世界发布‘度秘’，到2017年发布DuerOS并与硬件合作伙伴广泛合作，到2018年发布一系列小度智能硬件产品，百度旗下软硬件一体化的人工智能生态已经形成。小度目前涵盖小度系列智能硬件，小度助手软件服务（内置于第三方合作伙伴硬件及手机APP中），以及小度对话式人工智能操作系统。\n",
      "ASD阿波罗高阶智能驾驶\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "百度Apollo将多年积累的L4级自动驾驶能力应用在L2+智能驾驶产品上，目前已经具备量产能力搭载在多款乘用车上，为广大车主提供高级辅助驾驶产品。2023年，百度已经推出自主泊车Apollo Parking、高速行泊一体Apollo Highway Driving Pro、三域融通领航辅助驾驶Apollo City Driving Max组成的智驾产品矩阵。智驾产品已经在极越01、岚图FREE、广汽埃安全系、哈弗神兽、坦克500、欧拉闪电猫等多款明星车型上量产。2024年，基于自动驾驶大模型焕新升级的纯视觉高阶智能驾驶产品ASD（Apollo Self-Driving）即将在极越全系车型上量产首发。未来，搭载ASD的量产乘用车，通过来自百度Apollo训练的“安全靠谱好司机”，让更多用户享受到自动驾驶技术所带来的安全、舒适、高效的出行体验。 [165]\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "小度车载智慧助手\n",
      "小度车载智慧助手是依托百度Apollo智舱大模型，以文心大模型为基础经过专项模型训练、调优和知识增强，并与车载语音及座舱基础设施深度融合，专为智能汽车客户打造的座舱智能体，可显著增强汽车座舱内的人机交互的体验，为用户提供拟人化的智慧助手服务。 [166]\n",
      "百度车联网\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "百度车联网\n",
      "百度车联网是Apollo的先行军，是百度AI赋能车场景的建设者和核心出口，也是百度人工智能战略的核心重要组成部分。在业界有领先的语音语义、多模交互、驾驶员监测、车载信息安全等核心能力，提供全方位的车联网生态系统及服务，加速人工智能车场景领域的产品落地。\n",
      "智能小程序：智能连接人与信息、人与服务、人与万物的开放生态\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "智能小程序是基于百度App的一种全新应用形态，可实现一次开发多端运行，既可以在百度系APP，也可以在其他合作APP上运行，帮助开发者更广泛地获取流量。无需安装，即点即用，体验堪比App，用户能更快捷地获取到想要的服务和信息。\n",
      "百度手机助手：最具人气的应用商店\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "百度手机助手是Android手机的权威资源平台，分发市场份额连续十个季度排名市场第一，拥有最全最好的应用、游戏、壁纸资源，帮助用户在海量资源中精准搜索、高速下载、轻松管理，万千汇聚，一触即得。\n",
      "百度网盘：让美好永远相伴\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "百度网盘是百度推出的一项云存储服务，不仅为用户提供免费的存储空间，还可以将照片、视频、文档、通讯录等数据在移动设备和PC客户端之间跨平台同步和管理；百度网盘还支持添加好友、创建群组，并可跨终端随时随地进行分享。\n",
      "百度智能云\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "百度智能云\n",
      "百度旗下面向企业及开发者的智能云计算服务平台，作为百度AI战略的载体和源动力，百度智能云是百度大脑的云化，承载了百度大脑150+AI能力，并且为Apollo和DuerOS提供技术支撑。\n",
      "百度移动端输入法：更懂你的表达\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "9亿用户在用的手机输入法，多次荣获「年度优秀应用」「最受欢迎输入法」等奖项。超大词库，智能联想出词，支持多种输入方式，打字流畅；语音输入高速、精准，支持中英自由说、方言自由说。提供千款个性化皮肤、emoji、颜文字，热门流行表情图每日更新，帮助年轻用户个性化的表达。\n",
      "百度浏览器：做个有趣的人\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "百度手机浏览器是百度自主研发，为手机上网用户量身定制的一款浏览类产品，于2011 年6月15日正式上线公测，极速内核强劲动力，提供超强智能搜索，整合百度优质服务。\n",
      "Hao123：上网从这里开始\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Hao123创立于1999年，2004年被百度收购。作为百度旗下核心产品，hao123及时收录包括音乐、视频、小说、游戏等热门分类的网站，与搜索完美结合，为中国互联网用户提供最简单便捷的网上导航服务，重新定义了上网导航的概念。\n",
      "百度安全：有AI更安全\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "百度安全是百度公司旗下，以AI为核心、大数据为基础打造的领先安全品牌，是百度在互联网安全18年最佳实践的总结与提炼。业务由AI安全、移动安全、云安全、数据安全、业务安全五大矩阵构成，全面覆盖百度各种复杂业务场景，同时向个人用户和商业伙伴输出领先的安全产品与行业一体化解决方案。\n",
      "百度商业服务，新生产力引擎\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "百度商业服务整合了搜索、资讯、视频、线下场景屏、联盟流量等资源，形成全场景全用户覆盖的媒体矩阵，并依托AI技术和大数据能力提供消费者洞察、自动化创意、商家小程序等一整套智能营销解决方案。为企业提供品牌建设、效果推广及消费者运营的全方位商业服务。\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "2019年数字营销将进入以AI智能营销为核心的4.0时代，百度商业服务将实现新连接、新场景、新流量、新品牌的\"四新合力\"升级，并全面应用于搜索推广、信息流广告、品牌营销、商品推广等核心商业产品。\n",
      "全新搜索推广：从流量运营到消费者运营，在意图识别、流量、创意、广告匹配等全链路应用AI能力，打通线上线下场景实现用户场景全覆盖，并继续为所有的大客户和中小本地客户提供营销和消费者运营能力。\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "信息流广告：与搜索推广形成“搜索+推荐”双引擎，全面应用AI技术的同时，整合百度APP、好看视频、全民小视频等原生流量及优质的联盟流量资源，支持图片、视频、AR等多种形式的广告创意，并提供便捷易用的营销工具，为企业提供一站式的信息流营销服务。\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "品牌营销：聚合线上线下资源，为商业伙伴的品牌成长提供平台支撑，用大数据赋能品牌营销，用AI实现心智营销，用百度全意识整合营销数字平台Omni Marketing帮助品牌在品牌建设、品牌沟通、品牌转化中实现品效协同，精准有效地发现目标消费者，通过恰当媒体与消费者沟通，最终实现消费者的有效转化。\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "闪投商品推广（DPA：Dynamic Product Ads）：AI驱动的千人千面的精准营销，智能连接人与商品，基于大数据刻画人群画像并进行意图的动态识别，基于结构化的商品数据进行商品与意图的匹配并实现广告创意的智能拼接，从而实现在正确的时间和场景向正确的人正确地展示正确的商品信息。\n",
      "---------\n",
      "Question: 这个文档在说什么？\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not parse output: <think>\n好的，我现在需要回答用户的问题：“这个文档在说什么？”首先，我得仔细阅读提供的上下文内容。\n\n上下文中提到百度Apollo将L4级自动驾驶能力应用到L2+智能驾驶产品上，并且已经量产多款乘用车。此外，2023年他们推出了多个智驾产品矩阵，如Apollo Parking、Highway Driving Pro和City Driving Max，这些产品已经在多款明星车型上量产。\n\n接下来，我需要理解文档的主要内容。文档主要讲述百度Apollo在自动驾驶领域的应用进展，包括技术升级和量产情况，并且提到未来ASD产品的计划。因此，文档的核心内容是关于百度Apollo在自动驾驶方面的技术发展及其在汽车制造中的应用。\n\n用户的问题是询问这个文档的内容，所以我的回答应该涵盖文档的主要信息，包括百度Apollo的技术应用、产品矩阵的推出以及未来的计划。同时，我需要确保回答准确且全面，不遗漏任何关键点。\n</think>\n\nThis document discusses the development and application of百度 Apollo's L4-level autonomous driving technology in its L2+ intelligent driving products. It highlights that百度 Apollo has already produced several models, includingApollo Parking, Highway Driving Pro, and City Driving Max, which are available on major car manufacturers like极越01、岚图FREE等. Additionally, the document mentions that a new high-order self-driving product called ASD (Apollo Self-Driving) is set to be released in the future, based on a newly enhanced large model, with plans for it to launch across the market's top models first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33m这个文档在说什么？\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result = \u001b[43mqa_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_documents\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43msplit_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain_core/_api/deprecation.py:181\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    180\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py:389\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    358\u001b[39m \n\u001b[32m    359\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    380\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    381\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    382\u001b[39m config = {\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    384\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    385\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    386\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    387\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py:170\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    169\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    171\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py:160\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    159\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    163\u001b[39m     )\n\u001b[32m    165\u001b[39m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    166\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    167\u001b[39m     )\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain/chains/combine_documents/base.py:138\u001b[39m, in \u001b[36mBaseCombineDocumentsChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[32m    137\u001b[39m other_keys = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items() \u001b[38;5;28;01mif\u001b[39;00m k != \u001b[38;5;28mself\u001b[39m.input_key}\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m output, extra_return_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_run_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mother_keys\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m extra_return_dict[\u001b[38;5;28mself\u001b[39m.output_key] = output\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain/chains/combine_documents/map_rerank.py:182\u001b[39m, in \u001b[36mMapRerankDocumentsChain.combine_docs\u001b[39m\u001b[34m(self, docs, callbacks, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcombine_docs\u001b[39m(\n\u001b[32m    166\u001b[39m     \u001b[38;5;28mself\u001b[39m, docs: List[Document], callbacks: Callbacks = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any\n\u001b[32m    167\u001b[39m ) -> Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m    168\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Combine documents in a map rerank manner.\u001b[39;00m\n\u001b[32m    169\u001b[39m \n\u001b[32m    170\u001b[39m \u001b[33;03m    Combine by mapping first chain over all documents, then reranking the results.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    180\u001b[39m \u001b[33;03m        element returned is a dictionary of other keys to return.\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_and_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# FYI - this is parallelized and so it is fast.\u001b[39;49;00m\n\u001b[32m    184\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpage_content\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_results(docs, results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain/chains/llm.py:374\u001b[39m, in \u001b[36mLLMChain.apply_and_parse\u001b[39m\u001b[34m(self, input_list, callbacks)\u001b[39m\n\u001b[32m    369\u001b[39m warnings.warn(\n\u001b[32m    370\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe apply_and_parse method is deprecated, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    371\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minstead pass an output parser directly to LLMChain.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    372\u001b[39m )\n\u001b[32m    373\u001b[39m result = \u001b[38;5;28mself\u001b[39m.apply(input_list, callbacks=callbacks)\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain/chains/llm.py:381\u001b[39m, in \u001b[36mLLMChain._parse_generation\u001b[39m\u001b[34m(self, generation)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_parse_generation\u001b[39m(\n\u001b[32m    377\u001b[39m     \u001b[38;5;28mself\u001b[39m, generation: List[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]\n\u001b[32m    378\u001b[39m ) -> Sequence[Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prompt.output_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    380\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput_parser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m generation\n\u001b[32m    383\u001b[39m         ]\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    385\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.13/site-packages/langchain/output_parsers/regex.py:35\u001b[39m, in \u001b[36mRegexParser.parse\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.default_output_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not parse output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     38\u001b[39m             key: text \u001b[38;5;28;01mif\u001b[39;00m key == \u001b[38;5;28mself\u001b[39m.default_output_key \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     39\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output_keys\n\u001b[32m     40\u001b[39m         }\n",
      "\u001b[31mValueError\u001b[39m: Could not parse output: <think>\n好的，我现在需要回答用户的问题：“这个文档在说什么？”首先，我得仔细阅读提供的上下文内容。\n\n上下文中提到百度Apollo将L4级自动驾驶能力应用到L2+智能驾驶产品上，并且已经量产多款乘用车。此外，2023年他们推出了多个智驾产品矩阵，如Apollo Parking、Highway Driving Pro和City Driving Max，这些产品已经在多款明星车型上量产。\n\n接下来，我需要理解文档的主要内容。文档主要讲述百度Apollo在自动驾驶领域的应用进展，包括技术升级和量产情况，并且提到未来ASD产品的计划。因此，文档的核心内容是关于百度Apollo在自动驾驶方面的技术发展及其在汽车制造中的应用。\n\n用户的问题是询问这个文档的内容，所以我的回答应该涵盖文档的主要信息，包括百度Apollo的技术应用、产品矩阵的推出以及未来的计划。同时，我需要确保回答准确且全面，不遗漏任何关键点。\n</think>\n\nThis document discusses the development and application of百度 Apollo's L4-level autonomous driving technology in its L2+ intelligent driving products. It highlights that百度 Apollo has already produced several models, includingApollo Parking, Highway Driving Pro, and City Driving Max, which are available on major car manufacturers like极越01、岚图FREE等. Additionally, the document mentions that a new high-order self-driving product called ASD (Apollo Self-Driving) is set to be released in the future, based on a newly enhanced large model, with plans for it to launch across the market's top models first."
     ]
    }
   ],
   "source": [
    "query = \"这个文档在说什么？\"\n",
    "result = qa_chain({\"input_documents\":split_docs, \"question\":query})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
