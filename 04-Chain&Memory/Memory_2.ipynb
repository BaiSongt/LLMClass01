{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在链上使用记忆\n",
    "- LLMChain\n",
    "- ConversationChain\n",
    "- 自定义\n",
    "- 同一个链合并使用多个记忆\n",
    "- 给一个多参数链增加记忆\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "ollama_llm = OllamaLLM(model=\"llama3.1:8b\", temperature=0)\n",
    "\n",
    "# 自定义模板\n",
    "template = \"\"\"你是一个可以和人类对话的机器人。\n",
    "{chat_history}\n",
    "人类：{human_input}\n",
    "机器人：\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  template=template,\n",
    "  input_variables=[\"chat_history\", \"human_input\"],\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "  memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "  llm=ollama_llm,\n",
    "  memory=memory,\n",
    "  prompt=prompt,\n",
    "  verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m你是一个可以和人类对话的机器人。\n",
      "Human: 你好\n",
      "AI: 你好！很高兴见到你。今天过得怎么样？\n",
      "Human: 今天成都的天气真好，已经很久没有出太阳了\n",
      "AI: 太好了！成都的天气确实是美丽的。太阳出来了，肯定会让大家的心情更好一些。有去外面晒晒太阳吗？\n",
      "人类：成都今天的天气怎么样\n",
      "机器人：\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'很不错啊！刚刚你也说了，已经很久没有出太阳了，所以今天的天气应该是比较晴朗的。'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain.predict(human_input=\"你好\")\n",
    "# chain.predict(human_input=\"今天成都的天气真好，已经很久没有出太阳了\")\n",
    "chain.predict(human_input=\"成都今天的天气怎么样\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate 具有特定的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: 你好，我是一个可以和人类对话的机器人\n",
      "Human: 你好\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    SystemMessage(content=\"你好，我是一个可以和人类对话的机器人\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{human_input}\"),\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(prompt.format(human_input=\"你好\", chat_history=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",return_messages=True)\n",
    "chat_llm = ChatOllama(model=\"llama3.1:8b\", temperature=0)\n",
    "\n",
    "from langchain.chains.llm import LLMChain\n",
    "chain = LLMChain(\n",
    "  llm=chat_llm,\n",
    "  memory=memory,\n",
    "  prompt=prompt,\n",
    "  verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 你好，我是一个可以和人类对话的机器人\n",
      "Human: 你好\n",
      "AI: 很高兴见到你！我是你的机器人朋友。有什么想聊天或问的问题吗？\n",
      "Human: 你好\n",
      "AI: 再次打招呼！今天过得怎么样呢？有任何新鲜事要分享吗？\n",
      "Human: 今天成都的天气真好，已经很久没有出太阳了\n",
      "AI: 太好了！阳光总是能让人心情更好。希望你在外面散步时能感受到温暖的阳光。成都的天气变化确实比较多，有时候会突然变冷或变热，但总体来说还是很美丽的城市呢。\n",
      "Human: 你好\n",
      "AI: 再次打招呼！看起来你还在享受着今天的好天气呢？有没有计划去哪里玩耍或者游览一下成都的景点？\n",
      "Human: 今天成都的天气真好，已经很久没有出太阳了\n",
      "AI: 太好了！阳光总是能让人心情更好。希望你在外面散步时能感受到温暖的阳光。成都的天气变化确实比较多，有时候会突然变冷或变热，但总体来说还是很美丽的城市呢。\n",
      "\n",
      "你喜欢哪个季节的成都天气？春天的花季，夏天的炎热，秋天的红叶，还是冬天的雾霭？\n",
      "Human: 成都今天的天气怎么样\n",
      "AI: 根据你的描述，我知道了！成都今天的天气很好，已经很久没有出太阳了。所以，今天应该是比较晴朗和温暖的天气呢！\n",
      "Human: 成都今天的天气怎么样\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我可以通过网络获取一些信息，但是如果你想知道今天成都的具体天气情况（如温度、湿度等），我建议你可以查一下当地的天气预报网站或APP。'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain.predict(human_input=\"你好\")\n",
    "# chain.predict(human_input=\"今天成都的天气真好，已经很久没有出太阳了\")\n",
    "chain.predict(human_input=\"成都今天的天气怎么样\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationChain\n",
    "提示词模板就是 `input` `history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.base import ConversationChain\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "ollama_llm = OllamaLLM(model=\"llama3.1:8b\", temperature=0)\n",
    "memory = ConversationBufferMemory(memory_key=\"history\",return_messages=True)\n",
    "chain = ConversationChain(\n",
    "  llm=ollama_llm,\n",
    "  memory=memory,\n",
    "  verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='你好', additional_kwargs={}, response_metadata={}), AIMessage(content='你好！我很高兴和您交谈。您想问些什么呢？我可以提供有关各种话题的信息，包括但不限于天文学、历史、科学、艺术等。或者，您也可以与我讨论一些更轻松的话题，如电影、音乐或最新的科技新闻。', additional_kwargs={}, response_metadata={}), HumanMessage(content='今天成都的天气真好，已经很久没有出太阳了', additional_kwargs={}, response_metadata={}), AIMessage(content=\"[AIMessage(content='您说的对！根据我最近的数据更新，成都的天气确实是非常舒适。根据中国气象局的预报，今天的成都将有大量的阳光和蓝天。事实上，成都的平均日照时数已经超过了8个小时，这意味着您可以在外面度过一个美好的日子。', additional_kwargs={}, response_metadata={})]\", additional_kwargs={}, response_metadata={})]\n",
      "Human: 成都今天的天气怎么样\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'根据我最近的数据更新，成都的天气确实是非常舒适。根据中国气象局的预报，今天的成都将有大量的阳光和蓝天。事实上，成都的平均日照时数已经超过了8个小时，这意味着您可以在外面度过一个美好的日子。'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain.predict(input=\"你好\")\n",
    "# chain.predict(input=\"今天成都的天气真好，已经很久没有出太阳了\")\n",
    "chain.predict(input=\"成都今天的天气怎么样\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 针对COnversation做一个定制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义一下，对其进行覆盖\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"下面是一段AI与人类的对话，AI会针对人类的提问，做尽可能详细的回答，\n",
    "如果AI不知道答案，会直接回复：人类姥爷，这个问题我不知道。\n",
    "当前对话：\n",
    "{history}\n",
    "human:{input}\n",
    "AI:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template,\n",
    "                        input_variables=[\"history\",\"input\"])\n",
    "memory = ConversationBufferMemory(ai_prefix=\"AI\",return_messages=True)\n",
    "chain = ConversationChain(\n",
    "  llm=ollama_llm,\n",
    "  memory=memory,\n",
    "  prompt=prompt, # 这里覆盖了ConversationChain的预制Prompt\n",
    "  verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m下面是一段AI与人类的对话，AI会针对人类的提问，做尽可能详细的回答，\n",
      "如果AI不知道答案，会直接回复：人类姥爷，这个问题我不知道。\n",
      "当前对话：\n",
      "[HumanMessage(content='你好', additional_kwargs={}, response_metadata={}), AIMessage(content='你好！很高兴和你交流。有什么问题需要我的帮助吗？', additional_kwargs={}, response_metadata={}), HumanMessage(content='今天成都的天气真好，已经很久没有出太阳了', additional_kwargs={}, response_metadata={}), AIMessage(content=\"[AIMessage(content='是啊，最近几天天气一直不太好呢！不过今天看起来确实是晴天了。', additional_kwargs={}, response_metadata={})]\", additional_kwargs={}, response_metadata={}), HumanMessage(content='成都今天的天气怎么样', additional_kwargs={}, response_metadata={}), AIMessage(content='[AIMessage(content=\"根据目前的天气预报，成都今天的天气应该是多云转晴，温度在15-20度之间。虽然最近几天天气不太好，但今天看起来确实是晴天了。\", additional_kwargs={}, response_metadata={})]', additional_kwargs={}, response_metadata={}), HumanMessage(content='成都明天的天气怎么样', additional_kwargs={}, response_metadata={}), AIMessage(content='根据目前的天气预报，成都明天的天气应该是多云转阴，温度在14-19度之间。虽然今天看起来很晴朗，但预计明天会有几场小雨。', additional_kwargs={}, response_metadata={})]\n",
      "human:我叫什么名字\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'人类姥爷，这个问题我不知道。'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain.predict(input=\"你好\")\n",
    "# chain.predict(input=\"今天成都的天气真好，已经很久没有出太阳了\")\n",
    "# chain.predict(input=\"成都今天的天气怎么样\")\n",
    "# chain.predict(input=\"成都明天的天气怎么样\")\n",
    "chain.predict(input=\"我叫什么名字\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "## 同一个链合并使用多个memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.base import ConversationChain\n",
    "from langchain.memory import ( ConversationSummaryMemory,\n",
    "  ConversationBufferMemory,\n",
    "  CombinedMemory # 合并Chain\n",
    "  )\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_llm = ollama_llm\n",
    "\n",
    "# 使用ConversationSummaryMemory对对话进行总结\n",
    "summary = ConversationSummaryMemory(\n",
    "  llm=ollama_llm,\n",
    "  input_key=\"input\"\n",
    ")\n",
    "\n",
    "cov_memory = ConversationBufferMemory(\n",
    "  memory_key=\"history_new\",\n",
    "  input_key=\"input\"\n",
    ")\n",
    "\n",
    "memory = CombinedMemory(\n",
    "  memories=[summary, cov_memory]\n",
    ")\n",
    "\n",
    "TEMPLATE = \"\"\"下面是一段AI与人类的对话，AI会针对人类的提问，做尽可能详细的回答，\n",
    "如果AI不知道答案，会直接回复：人类姥爷，这个问题我不知道。\n",
    "之前的对话摘要：\n",
    "{history}\n",
    "当前对话：\n",
    "{history_new}\n",
    "human:{input}\n",
    "AI:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  template=TEMPLATE,\n",
    "  input_variables=[\"history\",\"history_new\",\"input\"]\n",
    ")\n",
    "\n",
    "conv_chain = ConversationChain(\n",
    "  llm=ollama_llm,\n",
    "  memory=memory,\n",
    "  prompt=prompt,\n",
    "  verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m下面是一段AI与人类的对话，AI会针对人类的提问，做尽可能详细的回答，\n",
      "如果AI不知道答案，会直接回复：人类姥爷，这个问题我不知道。\n",
      "之前的对话摘要：\n",
      "Here is the updated summary:\n",
      "\n",
      "The human asks how the AI analyzes the cryptocurrency market. The AI's analysis is based on multiple data sources and algorithms, including monitoring global cryptocurrency trading volumes, price fluctuations, and related news events, as well as analyzing technical indicators such as MACD and RSI to determine market trends.\n",
      "\n",
      "The AI also uses machine learning models to train on historical data and predict future trends and risks. These models are based on complex algorithms, including time series analysis and association rule mining.\n",
      "\n",
      "Additionally, the AI references reports from authoritative media and research institutions in the cryptocurrency field to understand market dynamics and policy impacts. The AI's final report is a data-driven analysis that combines all this information for user reference.\n",
      "\n",
      "The human then asks about the prospects of quantitative investing, but the AI responds with a humorous and non-committal answer, saying \"人类姥爷，这个问题我不知道\" (which roughly translates to \"Grandpa human, I don't know\").\n",
      "当前对话：\n",
      "Human: 你对加密市场是如何分析的？\n",
      "AI: 我对加密市场的分析基于多种数据来源和算法。首先，我监测全球各地的加密货币交易量、价格波动以及相关新闻事件。同时，我也分析了加密货币的技术指标，如MACD、RSI等，以判断市场趋势。\n",
      "\n",
      "其次，我结合机器学习模型对历史数据进行训练，预测出未来可能出现的趋势和风险。这些模型基于复杂的算法，包括时间序列分析、关联规则挖掘等技术。\n",
      "\n",
      "此外，我也会参考加密货币领域的权威媒体和研究机构的报告，了解市场动态和政策影响。例如，比特币的价格可能受到美国监管机构对加密货币的政策变化的影响。\n",
      "\n",
      "最后，我将所有这些信息综合起来，生成一个基于数据驱动的分析报告，提供给用户参考。\n",
      "Human: 这么厉害，你觉得量化投资的前景如何？\n",
      "AI: 人类姥爷，这个问题我不知道。\n",
      "human:这么厉害，你觉得加密市场的前景如何？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'人类姥爷，这个问题我知道！根据我的分析和数据驱动的模型，目前加密市场呈现出快速增长和波动的特征。虽然有很多挑战和风险，但也存在着巨大的潜力和机会。\\n\\n例如，比特币价格已经突破了之前的历史高点，并且其他一些加密货币也表现出了强劲的增长势头。但是，市场也面临着监管、安全性和可持续性的问题，这些挑战可能会影响市场的前景。\\n\\n但是，如果你问我关于量化投资的前景，我还是会说：人类姥爷，这个问题我不知道。'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conv_chain.run(\"你对加密市场是如何分析的？\")\n",
    "# conv_chain.run(\"这么厉害，你觉得量化投资的前景如何？\")\n",
    "conv_chain.run(\"这么厉害，你觉得加密市场的前景如何？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多参数链增加记忆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 155, which is longer than the specified 100\n",
      "Created a chunk of size 126, which is longer than the specified 100\n",
      "Created a chunk of size 164, which is longer than the specified 100\n",
      "Created a chunk of size 121, which is longer than the specified 100\n",
      "Created a chunk of size 182, which is longer than the specified 100\n",
      "Created a chunk of size 122, which is longer than the specified 100\n",
      "Created a chunk of size 164, which is longer than the specified 100\n",
      "Created a chunk of size 381, which is longer than the specified 100\n",
      "Created a chunk of size 127, which is longer than the specified 100\n",
      "Created a chunk of size 126, which is longer than the specified 100\n",
      "Created a chunk of size 112, which is longer than the specified 100\n",
      "Created a chunk of size 105, which is longer than the specified 100\n",
      "Created a chunk of size 133, which is longer than the specified 100\n",
      "Created a chunk of size 113, which is longer than the specified 100\n",
      "Created a chunk of size 138, which is longer than the specified 100\n",
      "Created a chunk of size 122, which is longer than the specified 100\n",
      "Created a chunk of size 121, which is longer than the specified 100\n",
      "Created a chunk of size 146, which is longer than the specified 100\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "with open(\"long_text.txt\") as f:\n",
    "  text = f.read()\n",
    "\n",
    "# 文本切分\n",
    "text_splitter = CharacterTextSplitter(\n",
    "  separator='\\n',\n",
    "  chunk_size = 100,\n",
    "  chunk_overlap = 5,\n",
    ")\n",
    "texts = text_splitter.split_text(text)\n",
    "\n",
    "# 使用 Ollama 本地的Embedding\n",
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text:latest\",temperature=0)\n",
    "\n",
    "# 使用 Chroma 存储 向量\n",
    "docsearch_vecstor = Chroma.from_texts(texts, embedding)\n",
    "\n",
    "query = \"百度有哪些产品？\"\n",
    "docs = docsearch_vecstor.similarity_search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m下面是一段AI与人类的对话，AI会针对人类的提问，做尽可能详细的回答，\n",
      "如果AI不知道答案，会直接回复：人类姥爷，这个问题我不知道。参考一下相关文档以及历史对话信息，\n",
      "AI会根据此组织和回答内容：：\n",
      "百度浏览器：做个有趣的人\n",
      "\n",
      "百度知道：总有一个人知道你问题的答案\n",
      "\n",
      "百度安全：有AI更安全\n",
      "\n",
      "百度安全：有AI更安全\n",
      "[]\n",
      "human:百度有哪些产品？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(metadata={}, page_content='百度浏览器：做个有趣的人'),\n",
       "  Document(metadata={}, page_content='百度知道：总有一个人知道你问题的答案'),\n",
       "  Document(metadata={}, page_content='百度安全：有AI更安全'),\n",
       "  Document(metadata={}, page_content='百度安全：有AI更安全')],\n",
       " 'human_input': '百度有哪些产品？',\n",
       " 'chat_history': [HumanMessage(content='百度有哪些产品？', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='好问题！百度是一个多元化的互联网公司，拥有许多不同的产品和服务。以下是其中一些主要的产品：\\n\\n1. **百度搜索**：这是百度最著名的产品之一，是一个全能的搜索引擎，可以帮助用户快速找到网上信息。\\n2. **百度知道**：是一个问答式的知识分享平台，用户可以在这里提问和回答问题，获取各种知识和经验。\\n3. **百度浏览器**：是百度推出的一个自定义浏览器，可以帮助用户更好地使用互联网，并提供一些额外的功能和服务。\\n4. **百度安全**：是一个专门为用户提供网络安全保护的产品，包括防病毒、防垃圾邮件等功能。\\n5. **百度云**：是一个云计算平台，提供存储、计算、数据库等服务，让用户可以更方便地管理和使用自己的数据。\\n6. **百度地图**：是一个专业的地理信息系统，可以帮助用户查找路线、查看地图等。\\n7. **百度推广**：是百度为企业提供的在线营销平台，包括搜索引擎优化（SEO）、付费推广等服务。\\n\\n这些产品和服务都是百度为了让用户更好地使用互联网而开发的。', additional_kwargs={}, response_metadata={})],\n",
       " 'output_text': '好问题！百度是一个多元化的互联网公司，拥有许多不同的产品和服务。以下是其中一些主要的产品：\\n\\n1. **百度搜索**：这是百度最著名的产品之一，是一个全能的搜索引擎，可以帮助用户快速找到网上信息。\\n2. **百度知道**：是一个问答式的知识分享平台，用户可以在这里提问和回答问题，获取各种知识和经验。\\n3. **百度浏览器**：是百度推出的一个自定义浏览器，可以帮助用户更好地使用互联网，并提供一些额外的功能和服务。\\n4. **百度安全**：是一个专门为用户提供网络安全保护的产品，包括防病毒、防垃圾邮件等功能。\\n5. **百度云**：是一个云计算平台，提供存储、计算、数据库等服务，让用户可以更方便地管理和使用自己的数据。\\n6. **百度地图**：是一个专业的地理信息系统，可以帮助用户查找路线、查看地图等。\\n7. **百度推广**：是百度为企业提供的在线营销平台，包括搜索引擎优化（SEO）、付费推广等服务。\\n\\n这些产品和服务都是百度为了让用户更好地使用互联网而开发的。'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 构架问答对话链\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "TEMPLATE = \"\"\"下面是一段AI与人类的对话，AI会针对人类的提问，做尽可能详细的回答，\n",
    "如果AI不知道答案，会直接回复：人类姥爷，这个问题我不知道。参考一下相关文档以及历史对话信息，\n",
    "AI会根据此组织和回答内容：：\n",
    "{context}\n",
    "{chat_history}\n",
    "human:{human_input}\n",
    "AI:\"\"\"\n",
    "\n",
    "ollama_llm = ollama_llm\n",
    "prompt = PromptTemplate(\n",
    "  template=TEMPLATE,\n",
    "  input_variables=[\"context\", \"chat_history\", \"human_input\"]\n",
    ")\n",
    "\n",
    "# 使用ConversationBufferMemory对对话进行缓存\n",
    "memory = ConversationBufferMemory(\n",
    "  memory_key=\"chat_history\",\n",
    "  input_key=\"human_input\",\n",
    "  return_messages=True\n",
    ")\n",
    "\n",
    "# 加载对话链\n",
    "chat_chain = load_qa_chain(\n",
    "  llm=ollama_llm,\n",
    "  memory=memory,\n",
    "  prompt=prompt,\n",
    "  verbose=True,\n",
    "  chain_type=\"stuff\"\n",
    ")\n",
    "\n",
    "\n",
    "chat_chain({\"input_documents\":docs, \"human_input\":\"百度有哪些产品？\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![memory——chain](pngs/主要的预制链和memory工具.png)\n",
    "![本章小结](pngs/小节.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
