{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境搭建\n",
    "- 安装pyenv brew安装\n",
    "- 将本地目录设置为需要的python版本\n",
    "- 安装LangChain包\n",
    "\n",
    "## 环境测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --upgrade langchain -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 依赖安装\n",
    "- 安装 openai api\n",
    "- 或者使用BaiChuan 、ChatGLM、 Deepseek 等开源包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install deepseek\n",
    "! pip3 install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install fastapi uvicorn requests python-multipart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastapi import FastAPI, HTTPException, Depends, Header\n",
    "import requests\n",
    "\n",
    "app = FastAPI()\n",
    "API_KEYS = {\"your_secret_key_123\"}\n",
    "\n",
    "def verify_key(api_key: str = Header(...)):\n",
    "    if api_key not in API_KEYS:\n",
    "        raise HTTPException(status_code=403)\n",
    "    return api_key\n",
    "\n",
    "@app.post(\"/api/generate\", dependencies=[Depends(verify_key)])\n",
    "async def generate(request_data: dict):\n",
    "    response = requests.post(\"http://localhost:11434/api/generate\", json=request_data)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "404 page not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m API_KEYS = {\u001b[33m\"\u001b[39m\u001b[33mOLLAMA_DEEPSEEK_API_KEY\u001b[39m\u001b[33m\"\u001b[39m}     \u001b[38;5;66;03m# 自定义合法 API Key\u001b[39;00m\n\u001b[32m     10\u001b[39m client = OpenAI(api_key=API_KEYS, base_url=OLLAMA_URL)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdeepseek-r1:7b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHello\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/AI_Class/lib/python3.12/site-packages/openai/_utils/_utils.py:279\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/AI_Class/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:879\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    837\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    839\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    876\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    877\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    878\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/AI_Class/lib/python3.12/site-packages/openai/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/AI_Class/lib/python3.12/site-packages/openai/_base_client.py:919\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    917\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/AI_Class/lib/python3.12/site-packages/openai/_base_client.py:1023\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1020\u001b[39m         err.response.read()\n\u001b[32m   1022\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1026\u001b[39m     cast_to=cast_to,\n\u001b[32m   1027\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1031\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1032\u001b[39m )\n",
      "\u001b[31mNotFoundError\u001b[39m: 404 page not found"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# ds_api_key = os.getenv(\"OLLAMA_API_KEY\")\n",
    "# ds_url = os.getenv(\"OLLANA_PROXY_ADRESS\")\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434\"  # Ollama 默认地址\n",
    "API_KEYS = {\"OLLAMA_DEEPSEEK_API_KEY\"}     # 自定义合法 API Key\n",
    "\n",
    "client = OpenAI(api_key=API_KEYS, base_url=OLLAMA_URL)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-r1:7b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='deepseek-r1:7b' created_at='2025-03-10T07:02:13.70544Z' done=True done_reason='stop' total_duration=7383996375 load_duration=27003416 prompt_eval_count=5 prompt_eval_duration=326000000 eval_count=75 eval_duration=7028000000 response='<think>\\n您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。\\n</think>\\n\\n您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。' context=[151644, 109432, 107828, 151645, 151648, 198, 111308, 6313, 104198, 67071, 105538, 102217, 30918, 50984, 9909, 33464, 39350, 7552, 73218, 100013, 9370, 100168, 110498, 33464, 39350, 10911, 16, 1773, 29524, 87026, 110117, 99885, 86119, 3837, 105351, 99739, 35946, 111079, 113445, 100364, 8997, 151649, 271, 111308, 6313, 104198, 67071, 105538, 102217, 30918, 50984, 9909, 33464, 39350, 7552, 73218, 100013, 9370, 100168, 110498, 33464, 39350, 10911, 16, 1773, 29524, 87026, 110117, 99885, 86119, 3837, 105351, 99739, 35946, 111079, 113445, 100364, 1773]\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# 简单文本生成\n",
    "response = ollama.generate(\n",
    "    model=\"deepseek-r1:7b\",  # 模型名称或本地路径\n",
    "    prompt=\"介绍一下你自己\",\n",
    "    # temperature=0.7,\n",
    "    # max_tokens=100\n",
    ")\n",
    "print(response)\n",
    "\n",
    "# 对话模式\n",
    "# response = ollama.chat(\n",
    "#     model=\"llama3.1:8b\",\n",
    "#     messages=[{\"role\": \"user\", \"content\": \"请解释量子计算原理\"}]\n",
    "# )\n",
    "# print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 通过LangChain调用 ollama LLMs\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      7\u001b[39m OLLAMA_URL = \u001b[33m\"\u001b[39m\u001b[33mhttp://localhost:11434\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Ollama 默认地址\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/AI_Class/lib/python3.12/site-packages/langchain/llms/__init__.py:545\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m llms\n\u001b[32m    547\u001b[39m     \u001b[38;5;66;03m# If not in interactive env, raise warning.\u001b[39;00m\n\u001b[32m    548\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_interactive_env():\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "# 通过LangChain调用 ollama LLMs\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434\"  # Ollama 默认地址\n",
    "API_KEYS = {\"OLLAMA_DEEPSEEK_API_KEY\"}     # 自定义合法 API Key\n",
    "\n",
    "llm = OpenAI(\n",
    "  model=\"deepseek-r1:7b\",\n",
    "  temperature=0,\n",
    "  openai_api_key=API_KEYS,\n",
    "  openai_api_base=OLLAMA_URL\n",
    ")\n",
    "\n",
    "llm.invoke(\"你好\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过 langchain 调用 ollama 本地 大模型\n",
    "! pip3 install langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<think>\\n好的，我现在需要回答关于有限元法（FEM）的基本思想的问题。首先，我应该回忆一下有限元方法是什么，它是由谁提出的，是哪里发展起来的，以及它的主要原理和应用领域。\\n\\n我记得有限元方法起源于20世纪40年代，特别是美国的一些工程家，比如罗宾·克勒尼森（Robin Cottonto）和拉尔夫·卡特里安·斯通（Raymond F. Smith）。他们专注于飞机设计，但他们的研究没有得到广泛应用。后来，随着计算机技术的发展，有限元法被广泛应用于各个领域。\\n\\n接下来，我想了解有限元法的基本原理。它应该是一种数值方法，将复杂的结构分解为许多简单的部分，称为元素，然后分别分析每个元素的行为，最后把这些结果组合起来得到整个结构的解。这种方法的核心在于离散化和求解。\\n\\n离散化指的是如何将连续的物理域转换成离散的、有限的节点集合。这里可能涉及到坐标变换，把原始几何映射到某个标准形状的单元（比如三角形或四边形）。然后用插值函数来近似场变量在每个节点上的值，通过这些值建立方程组。\\n\\n求解部分包括建立和求解线性或者非线性代数方程组。因为有限元法是数值方法，所以问题通常需要 iterative 的过程，比如牛顿-拉夫逊方法或外推法来逼近精确解。同时，离散化的精度必须足够高以确保结果的准确性。\\n\\n应用领域方面，有限元法广泛应用于工程和科学研究，特别是在力学、热学、电磁场、材料科学等领域。例如，在机械设计中用于结构优化；在土木工程中用于建筑物的安全评估；在航空航天中用于飞行器的设计和分析。\\n\\n我还需要考虑一些关键点：有限元法的离散化是否总是精确？现实中通常需要一些近似，所以理解这点很重要。另外，方程组建立的过程可能涉及复杂的积分运算，尤其是流体动力学中的流体流动问题可能比较复杂。\\n\\n总结一下，有限元法的基本思想是将连续的问题转化为离散的问题，通过数值方法求解，并在应用中广泛使用以解决各种工程和科学问题。\\n</think>\\n\\n有限元法（Finite Element Method, FEM）是一种数值分析技术，主要用于将复杂的结构问题转化为离散的节点集合。其基本原理包括：\\n\\n1. **离散化**：将连续的物理域分解为许多简单的元素，例如三角形或四边形。\\n\\n2. **插值与近似**：使用插值函数近似场变量在每个节点上的值，建立代数方程组。\\n\\n3. **求解代数方程组**：通过迭代方法（如牛顿-拉夫逊法）求解线性或非线性方程组，得到结构的响应。\\n\\n应用领域涵盖力学、热学、电磁场和材料科学等。总体而言，有限元法是将复杂问题转化为离散节点分析的过程，并利用数值计算求得解答。' additional_kwargs={} response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-03-10T07:23:06.389212Z', 'done': True, 'done_reason': 'stop', 'total_duration': 20384159791, 'load_duration': 9350996125, 'prompt_eval_count': 45, 'prompt_eval_duration': 707000000, 'eval_count': 649, 'eval_duration': 10324000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-506e8637-d0b2-4042-8f86-3180219553d9-0' usage_metadata={'input_tokens': 45, 'output_tokens': 649, 'total_tokens': 694}\n"
     ]
    }
   ],
   "source": [
    "# 通过 langchain 调用 ollama 本地 大模型\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 指定本地模型名称或路径\n",
    "ollama_llm = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\",  # 例如DeepSeek-R1 1.5B模型\n",
    "    base_url=\"http://localhost:11434\"  # Ollama默认地址\n",
    ")\n",
    "\n",
    "# 提示词模板\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    你是一个精通{topic}的专家，请根据提供的网页内容回答用户的问题：\n",
    "    {docs}\n",
    "    问题：{question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# 准备输入数据\n",
    "input_data = {\n",
    "    \"topic\": \"CAE有限元分析\",\n",
    "    \"docs\": \"网页内容字符串...\",\n",
    "    \"question\": \"有限元法的基本思想是什么？\"\n",
    "}\n",
    "\n",
    "# 执行链式调用\n",
    "response = (prompt_template | ollama_llm).invoke(input_data)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实际应用 起名字\n",
    "- LangChain 的第一个应用实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  你是一个起名大师, 请模仿示例起3-5个中国特色的名字，\n",
      "  如果是男孩叫做：狗蛋，如果是女孩叫做：翠花\n",
      "  \n",
      "<think>\n",
      "嗯，用户让我做一个起名大师，模仿示例生成三个到五个的中国特色名字。如果叫狗蛋或者翠花的话。首先，我得理解用户的需求是什么。他们可能是在写小说、故事或者需要一些名字来命名物品，比如宠物或角色。\n",
      "\n",
      "接下来，我要考虑用户的使用场景。可能是用于游戏、影视作品，或者是个人用途，比如给孩子起名。所以名字不仅要符合中国特色，还要有寓意，听起来顺口，容易记住。\n",
      "\n",
      "然后，我得分析示例中的名字“狗蛋”和“翠花”。狗蛋看起来像是一个宠物的名字，可能带有一种活泼、可爱的感觉；而翠花则带有春天的意象，可能给人一种清新、美丽的感觉。所以，我需要生成的名字也要有类似的寓意，同时还要符合中国特色的命名规则。\n",
      "\n",
      "接下来，我会考虑用户的性别，如果叫男孩的话，名字应该是狗蛋，如果是女孩的话是翠花。不过用户没有明确说明，所以我可能需要提供两种情况下的名字，或者只根据示例中的性别来判断。但为了保险起见，我可能会生成两个版本，一个男孩，一个女孩。\n",
      "\n",
      "然后，我会思考如何构造这些名字。可以从动物、植物、自然元素入手，比如鸟、鱼、草等等。同时，还要考虑名字的发音是否顺口，是否有押韵或者节奏感。比如“鸡”、“鸭”这样的动物名，听起来比较顺口；而“鱼”、“鸟”则可能更生动。\n",
      "\n",
      "另外，我还需要注意名字的地域特色，比如使用一些传统或有文化意义的词汇。比如“翠花”中的“花”带有春天的感觉，“狗蛋”中的“蛋”可能象征着蛋黄或者可爱。\n",
      "\n",
      "最后，我会确保每个名字都有一定的寓意和美感，同时符合中文的命名习惯。这样用户在使用时会更满意，也能增加名字的文化内涵。\n",
      "</think>\n",
      "\n",
      "当然可以！以下是三个到五个中国特色的名字，模仿了示例中的“狗蛋”（男孩）或“翠花”（女孩）的风格：\n",
      "\n",
      "1. **鸡蛋**  \n",
      "   - 意思：指一只小鸡。寓意活泼可爱，适合儿童使用。\n",
      "\n",
      "2. **鸭蛋**  \n",
      "   - 意思：指一只小鸭子。寓意灵动美丽，适合喜欢鸟类的孩子或宠物。\n",
      "\n",
      "3. **鱼蛋**  \n",
      "   - 意思：指一条小鱼。寓意轻盈自由，适合喜欢水中的动物的用户。\n",
      "\n",
      "4. **鸟蛋**  \n",
      "   - 意思：指一只小鸟。寓意生机勃勃，适合喜欢鸟类的家庭或个人。\n",
      "\n",
      "5. **草蛋**  \n",
      "   - 意思：指一片小草。寓意新生命，适合喜欢植物的用户。\n",
      "\n",
      "希望这些名字能符合你的需求！如果需要男孩或女孩的名字，请告诉我哦~\n"
     ]
    }
   ],
   "source": [
    "# 通过 langchain 调用 ollama 本地 大模型\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# 指定本地模型名称或路径\n",
    "ollama_llm = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\",  # 例如DeepSeek-R1 1.5B模型\n",
    "    base_url=\"http://localhost:11434\",  # Ollama默认地址\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# 提示词模板\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "  \"\"\"\n",
    "  你是一个起名大师, 请模仿示例起3-5个{country}的名字，\n",
    "  如果是男孩叫做：{boy_name}，如果是女孩叫做：{girl_name}\n",
    "  \"\"\"\n",
    ")\n",
    "\n",
    "message = prompt_template.format(country=\"中国特色\", boy_name=\"狗蛋\", girl_name=\"翠花\")\n",
    "print(message)\n",
    "\n",
    "# 执行链式调用\n",
    "# print(ollama_llm.invoke(message))\n",
    "print(ollama_llm.predict(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 格式化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 格式化LLM输出\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class NameFormatter(BaseOutputParser):\n",
    "    \"\"\"格式化LLM输出\"\"\"\n",
    "    def parse(self, text: str) -> dict:\n",
    "        return {\n",
    "            \"boy_name\": text.split(\"，\")[0],\n",
    "            \"girl_name\": text.split(\"，\")[1],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  你是一个起名大师, 请模仿示例起3-5个中国特色的名字，\n",
      "  姓:白,\n",
      "  如果是男孩叫做：白狗蛋，如果是女孩叫做：白翠花.\n",
      "  返回使用逗号分隔的列表形式，例如：[\"白狗蛋\", \"白翠花\"], 不需要返回其他内容。\n",
      "  \n",
      "content='<think>\\n嗯，用户让我做一个起名大师，模仿示例给他三个中国特色的名字。姓是白，如果叫男孩就是白狗蛋，女孩就是白翠花。然后要返回一个逗号分隔的列表。\\n\\n首先，我得理解用户的需求。他们可能需要一些英文名字，用于中文名字中，或者只是想得到一些英文版本。因为示例里用了英文名字，所以可能他们希望结果也是英文名。\\n\\n接下来，我要考虑用户的使用场景。可能是为某个项目、个人或家庭起名，比如孩子、宠物或者其他用途。个性化的名字很重要，所以要多样化，避免重复。\\n\\n然后，我需要分析用户提供的例子。白狗蛋和白翠花都是英文名字，看起来像是中文名字的变体。所以，我应该生成类似的英文名字，保持一定的规律性，但又不失独特性。\\n\\n接下来，我会思考不同的组合方式。比如，把“白”开头，后面加上其他字母或词，形成有意义的名字。考虑到中文文化中的元素，可能会加入一些吉祥字、成语或者传统元素。\\n\\n然后，我会考虑每个名字的结构和韵律。确保名字读起来顺口，发音和谐，同时符合英文名的规范。比如，使用常见的前缀如“Bo”、“Re”等，这样听起来更自然。\\n\\n最后，我会生成三个名字，每个都结合了中文元素和英文元素，避免重复，并且多样化。比如，“Bo Bo”（白狗），“Re Re”（红花），“Bo Re”（白红）。这样既符合中文文化，又显得独特。\\n\\n总结一下，我需要确保名字不仅有意义，还要符合用户的使用场景，同时保持多样性和创新性。\\n</think>\\n\\n[\"白狗蛋\", \"白翠花\", \"白Re\"]' additional_kwargs={} response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-03-10T07:58:55.629091Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5739895708, 'load_duration': 26950666, 'prompt_eval_count': 80, 'prompt_eval_duration': 229000000, 'eval_count': 369, 'eval_duration': 5483000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-379550bd-2f0e-4010-9a81-e1428996e9c3-0' usage_metadata={'input_tokens': 80, 'output_tokens': 369, 'total_tokens': 449}\n",
      "--------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AIMessage' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--------------------------------\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m#  format output using the output parser\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m name_list = \u001b[43mNameFormatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_out_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(name_list)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mNameFormatter.parse\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m，\u001b[39m\u001b[33m\"\u001b[39m),\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/AI_Class/lib/python3.12/site-packages/pydantic/main.py:891\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    888\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    889\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    890\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m891\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'AIMessage' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# 通过 langchain 调用 ollama 本地 大模型\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class NameFormatter(BaseOutputParser):\n",
    "    \"\"\"格式化LLM输出\"\"\"\n",
    "    def parse(self, text: str) -> dict:\n",
    "        return text.split(\"，\"),\n",
    "\n",
    "# 指定本地模型名称或路径\n",
    "ollama_llm = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\",  # 例如DeepSeek-R1 1.5B模型\n",
    "    base_url=\"http://localhost:11434\",  # Ollama默认地址\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# 提示词模板\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "  \"\"\"\n",
    "  你是一个起名大师, 请模仿示例起3-5个{country}的名字，\n",
    "  姓:{family_name},\n",
    "  如果是男孩叫做：{family_name}{boy_name}，如果是女孩叫做：{family_name}{girl_name}.\n",
    "  返回使用逗号分隔的列表形式，例如：[\"白狗蛋\", \"白翠花\"], 不需要返回其他内容。\n",
    "  \"\"\"\n",
    ")\n",
    "\n",
    "message = prompt_template.format(country=\"中国特色\",family_name=\"白\", boy_name=\"狗蛋\", girl_name=\"翠花\")\n",
    "print(message)\n",
    "\n",
    "# 执行链式调用\n",
    "# print(ollama_llm.invoke(message))\n",
    "llm_out_str = ollama_llm.invoke(message)\n",
    "print(llm_out_str)\n",
    "print(\"--------------------------------\")\n",
    "#  format output using the output parser\n",
    "name_list = NameFormatter().parse(llm_out_str)\n",
    "print(name_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
